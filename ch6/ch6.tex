\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Roots: Open Methods}
\label{chap:chap6}

\begin{center}
    \Large{\textbf{CHAPTER OBJECTIVES}}
\end{center}
The primary objective of this chapter is to acquaint you with open methods for finding
the root of a single nonlinear equation. Specific objectives and topics covered are

\begin{itemize}
    \item Recognizing the difference between bracketing and open methods for root
    location.
    \item Understanding the fixed-point iteration method and how you can evaluate its
    convergence characteristics.
    \item Knowing how to solve a roots problem with the Newton-Raphson method and
    appreciating the concept of quadratic convergence.
    \item Knowing how to implement both the secant and the modified secant methods.
    \item Understanding how Brent's method combines reliable bracketing methods with
    fast open methods to locate roots in a robust and efficient manner.
    \item Knowing how to use MATLAB's \texttt{fzero} function to estimate roots.
    \item Learning how to manipulate and determine the roots of polynomials with
    MATLAB.
\end{itemize}

\noindent For the bracketing methods in Chap. 5, the root is located within an interval prescribed
by a lower and an upper bound. Repeated application of these methods always results
in closer estimates of the true value of the root. Such methods are said to be \emph{convergent}
because they move closer to the truth as the computation progresses (Fig. 6.1a).

In contrast, the \emph{open methods} described in this chapter require only a single starting
value or two starting values that do not necessarily bracket the root. As such, they sometimes
\emph{diverge} or move away from the true root as the computation progresses (Fig. 6.1b).
However, when the open methods converge (Fig. 6.1c) they usually do so much more
quickly than the bracketing methods. We will begin our discussion of open techniques with
a simple approach that is useful for illustrating their general form and also for demonstrating
the concept of convergence.\\

\begin{figure}[h]
    \includegraphics[width = 0.5\linewidth]{./images/fig_6_1}
    \caption{Graphical depiction of the fundamental difference between the (a) bracketing and (b) and (c)
    open methods for root location. In (a), which is bisection, the root is constrained within the interval
    prescribed by $x_l$ and $x_u$ . In contrast, for the open method depicted in (b) and (c), which is
    Newton-Raphson, a formula is used to project from $x_i$ to $x_i+1$ in an iterative fashion. Thus the
    method can either (b) diverge or (c) converge rapidly, depending on the shape of the function
    and the value of the initial guess.}
\end{figure}
\bigskip

\section[SIMPLE FIXED-POINT ITERATION]{SIMPLE FIXED-POINT ITERATION}
\noindent As just mentioned, open methods employ a formula to predict the root. Such a formula can
be developed for simple \emph{fixed-point iteration} (or, as it is also called, \emph{one-point iteration} or
\emph{successive substitution}) by rearranging the function $f (x) = 0$ so that $x$ is on the left-hand
side of the equation:

$x = g(x)$
\hfill (6.1)\\

\noindent This transformation can be accomplished either by algebraic manipulation or by simply
adding $x$ to both sides of the original equation.

The utility of Eq. (6.1) is that it provides a formula to predict a new value of $x$ as a
function of an old value of $x$. Thus, given an initial guess at the root $x_i$ , Eq. (6.1) can be
used to compute a new estimate $x_{i+1}$ as expressed by the iterative formula\\

$x_{i+1} = g(x_i)$
\hfill (6.2)\\

\noindent As with many other iterative formulas in this book, the approximate error for this equation
can be determined using the error estimator:\\

$\epsilon_a = \left\lvert \dfrac{x_{i+1}-x_i}{x_{i+1}} \right\rvert 100\%$
\hfill (6.3)\\

\begin{example} Simple Fixed-Point Iteration\\
    
    \noindent\textbf{Problem Statement.}\quad Use simple fixed-point iteration to locate the root of
    $f(x) = e^{-x}-x$\\

    \noindent\textbf{Solution.} The function can be separated directly and expressed in the form of Eq. (6.2) as\\

    $x_{i+1} = e^{-x_i}$\\

    \noindent Starting with an initial guess of $x_0 = 0$, this iterative equation can be applied to compute:\\

    \begin{figure}[h]
        \includegraphics[width=0.6\linewidth]{./images/example_6_1_1}
    \end{figure}

    \noindent Thus, each iteration brings the estimate closer to the true value of the root: 0.56714329.
\end{example}

Notice that the true percent relative error for each iteration of Example 6.1 is roughly
proportional (for this case, by a factor of about 0.5 to 0.6) to the error from the previous
iteration. This property, called \emph{linear convergence}, is characteristic of fixed-point iteration.

Aside from the ``rate'' of convergence, we must comment at this point about the ``possibility'' 
of convergence. The concepts of convergence and divergence can be depicted
graphically. Recall that in Section 5.2, we graphed a function to visualize its structure and
behavior. Such an approach is employed in Fig. 6.2a for the function $f (x) = e^{-x} - x$ . An
alternative graphical approach is to separate the equation into two component parts, as in\\

$f_1(x)=f_2(x)$\\

\noindent Then the two equations\\

$y_1 = f_1(x)$
\hfill (6.4)\\

\noindent and\\

$y_2=f_2(x)$
\hfill (6.5)\\

\noindent can be plotted separately (Fig. 6.2b). The $x$ values corresponding to the intersections of
these functions represent the roots of $f (x) = 0$.

\begin{figure}[h]
    \includegraphics[width=0.4\linewidth]{./images/fig_6_2}
    \caption{Two alternative graphical methods for determining the root of $f (x) = e^{-x} - x$. (a) Root at the
    point where it crosses the x axis; (b) root at the intersection of the component functions.}
\end{figure}

The two-curve method can now be used to illustrate the convergence and divergence
of fixed-point iteration. First, Eq. (6.1) can be reexpressed as a pair of equations $y_1 = x$
and $y_2 = g(x)$. These two equations can then be plotted separately. As was the case with
Eqs. (6.4) and (6.5), the roots of $f (x) = 0$ correspond to the abscissa value at the intersection
of the two curves. The function $y_1 = x$ and four different shapes for $y_2 = g(x)$ are
plotted in Fig. 6.3.

For the first case (Fig. 6.3a), the initial guess of $x_0$ is used to determine the corresponding
point on the $y_2$ curve $[x_0, g(x_0)]$. The point $[x_1, x_1]$ is located by moving left horizontally to
the $y_1$ curve. These movements are equivalent to the first iteration of the fixed-point method:\\

$x_1 = g(x_0)$\\

\noindent Thus, in both the equation and in the plot, a starting value of $x_0$ is used to obtain an estimate
of $x_1$. The next iteration consists of moving to $[x_1, g(x_1)]$ and then to $[x_2, x_2]$. This
iteration is equivalent to the equation\\

$x_2 = g(x_1)$\\

\begin{figure}[h]
    \includegraphics[width=0.65\linewidth]{./images/fig_6_3}
    \caption{Graphical depiction of (a) and (b) convergence and (c) and (d) divergence of simple fixed-point
    iteration. Graphs (a) and (c) are called monotone patterns whereas (b) and (c) are called
    oscillating or spiral patterns. Note that convergence occurs when $\left\lvert g'(x) \right\rvert < 1$ }
\end{figure}

The solution in Fig. 6.3a is \emph{convergent} because the estimates of $x$ move closer to the
root with each iteration. The same is true for Fig. 6.3b. However, this is not the case for
Fig. 6.3c and d, where the iterations diverge from the root.

A theoretical derivation can be used to gain insight into the process. As described in
Chapra and Canale (2010), it can be shown that the error for any iteration is linearly proportional
to the error from the previous iteration multiplied by the absolute value of the
slope of $g$:\\

$E_{i+1} = g'(\xi)E_i$\\

\noindent Consequently, if $\left\lvert g' \right\rvert < 1$, the errors decrease with each iteration. For $\left\lvert g' \right\rvert > 1$ the errors
grow. Notice also that if the derivative is positive, the errors will be positive, and hence the
errors will have the same sign (Fig. 6.3a and c). If the derivative is negative, the errors will
change sign on each iteration (Fig. 6.3b and d).\\
\bigskip

\section[NEWTON-RAPHSON]{NEWTON-RAPHSON}
\noindent Perhaps the most widely used of all root-locating formulas is the \emph{Newton-Raphson method}
(Fig. 6.4). If the initial guess at the root is $x_i$, a tangent can be extended from the point
$[x_i , f (x_i )]$. The point where this tangent crosses the $x$ axis usually represents an improved
estimate of the root.

The Newton-Raphson method can be derived on the basis of this geometrical interpretation.
As in Fig. 6.4, the first derivative at $x$ is equivalent to the slope:\\

$f'(x_i) = \dfrac{f(x_i)-0}{x_i-x_{i+1}}$\\

\noindent which can be rearranged to yield\\

$x_{i+1} = x_i - \dfrac{f(x_i)}{f'(x_i)}$
\hfill (6.6)\\

\noindent which is called the \emph{Newton-Raphson formula.}\\

\begin{example} Newton-Raphson Method\\

    \noindent\textbf{Problem Statement}\quad Use the Newton-Raphson method to estimate the root of $f (x) =e^{-x} - x$
     employing an initial guess of $x_0 = 0$.\\

    \noindent\textbf{Solution.}\quad The first derivative of the function can be evaluated as\\

     $f'(x) = -e^{-x}-1$\\

    \noindent which can be substituted along with the original function into Eq. (6.6) to give\\

    $x_{i+1}= x_i-\dfrac{e^{-x_i}-x_i}{-e^{-x_i}-1}$\\

    \noindent Starting with an initial guess of $x_0 = 0$, this iterative equation can be applied to compute

    \begin{figure}[h]
        \includegraphics[width=0.4\linewidth]{./images/example_6_2_1}
    \end{figure}

    \noindent Thus, the approach rapidly converges on the true root. Notice that the true percent relative
    error at each iteration decreases much faster than it does in simple fixed-point iteration
    (compare with Example 6.1).
\end{example}

\begin{figure}[h]
    \includegraphics[width=0.55\linewidth]{./images/fig_6_4}
    \caption{Graphical depiction of the Newton-Raphson method. A tangent to the function of $x_i$ [that is,$f'(x)$] 
    is extrapolated down to the $x$ axis to provide an estimate of the root at $x_{i+1}$.}
\end{figure}

As with other root-location methods, Eq. (6.3) can be used as a termination criterion.
In addition, a theoretical analysis (Chapra and Canale, 2010) provides insight regarding the
rate of convergence as expressed by\\

$E_{t,i+1} = \dfrac{-f''(x_r)}{2f'(x_r)}E^2_{t,i}$
\hfill (6.7)\\

\noindent Thus, the error should be roughly proportional to the square of the previous error. In other
words, the number of significant figures of accuracy approximately doubles with each
iteration. This behavior is called \emph{quadratic convergence} and is one of the major reasons for
the popularity of the method.

Although the Newton-Raphson method is often very efficient, there are situations
where it performs poorly. A special case---multiple roots---is discussed elsewhere (Chapra
and Canale, 2010). However, even when dealing with simple roots, difficulties can also
arise, as in the following example.\\

\begin{example} A Slowly Converging Function with Newton-Raphson\\

    \noindent\textbf{Problem Statement.}\quad Determine the positive root of $f (x) = x^{10} - 1$ using the Newton-
    Raphson method and an initial guess of $x = 0.5$.\\

    \noindent\textbf{Solution.}\quad The Newton-Raphson formula for this case is\\
    
    $x_{i+1} = x_i - \dfrac{x^{10}_i - 1}{10x^9_i}$\\

    \noindent which can be used to compute\\

    \begin{figure}[h]
        \includegraphics[width=0.3\linewidth]{./images/example_6_3_1}
    \end{figure}

    \noindent Thus, after the first poor prediction, the technique is converging on the true root of 1, but
    at a very slow rate.

    Why does this happen? As shown in Fig. 6.5, a simple plot of the first few iterations is
    helpful in providing insight. Notice how the first guess is in a region where the slope is near
    zero. Thus, the first iteration flings the solution far away from the initial guess to a new
    value ($x = 51.65$) where $f (x)$ has an extremely high value. The solution then plods along
    for over 40 iterations until converging on the root with adequate accuracy.\\

    \begin{figure}[h]
        \includegraphics[width=0.6\linewidth]{./images/fig_6_5}
        \caption{Graphical depiction of the Newton-Raphson method for a case with slow convergence. The
        inset shows how a near-zero slope initially shoots the solution far from the root. Thereafter,
        the solution very slowly converges on the root.}
    \end{figure}
\end{example}

\begin{figure}[h]
    \includegraphics[width=0.5\linewidth]{./images/fig_6_6}
    \caption{Four cases where the Newton-Raphson method exhibits poor convergence.}
\end{figure}

Aside from slow convergence due to the nature of the function, other difficulties can
arise, as illustrated in Fig. 6.6. For example, Fig. 6.6a depicts the case where an inflection
point (i.e., $f'(x) = 0$) occurs in the vicinity of a root. Notice that iterations beginning at x0
progressively diverge from the root. Fig. 6.6b illustrates the tendency of the Newton-Raphson
technique to oscillate around a local maximum or minimum. Such oscillations may persist, or,
as in Fig. 6.6b, a near-zero slope is reached whereupon the solution is sent far from the area of
interest. Figure 6.6c shows how an initial guess that is close to one root can jump to a location
several roots away. This tendency to move away from the area of interest is due to the fact that
near-zero slopes are encountered. Obviously, a zero slope $[f'(x) = 0]$ is a real disaster because
it causes division by zero in the Newton-Raphson formula [Eq. (6.6)]. As in Fig. 6.6d,
it means that the solution shoots off horizontally and never hits the x axis.

Thus, there is no general convergence criterion for Newton-Raphson. Its convergence
depends on the nature of the function and on the accuracy of the initial guess. The only
remedy is to have an initial guess that is ``sufficiently'' close to the root. And for some functions,
no guess will work! Good guesses are usually predicated on knowledge of the physical
problem setting or on devices such as graphs that provide insight into the behavior of
the solution. It also suggests that good computer software should be designed to recognize
slow convergence or divergence.\\
\bigskip

\subsection{MATLAB M-file: newtraph}
\noindent An algorithm for the Newton-Raphson method can be easily developed (Fig. 6.7). Note that
the program must have access to the function (\texttt{func}) and its first derivative (\texttt{dfunc}). These
can be simply accomplished by the inclusion of user-defined functions to compute these
quantities. Alternatively, as in the algorithm in Fig. 6.7, they can be passed to the function
as arguments.

After the M-file is entered and saved, it can be invoked to solve for root. For example,
for the simple function $x^2 - 9$, the root can be determined as in\\

\texttt{>> newtraph(@(x) x\textasciicircum2-9,@(x) 2*x,5)\\
\indent ans =\\
\indent\indent 3\\}

\begin{example} Newton-Raphson Bungee Jumper Problem\\

    \noindent\textbf{Problem Statement.}\quad Use the M-file function from Fig. 6.7 to determine the mass of the
    bungee jumper with a drag coefficient of 0.25 kg/m to have a velocity of 36 m/s after 4 s of
    free fall. The acceleration of gravity is $9.81 m/s^2$.\\

    \noindent\textbf{Solution.}\quad The function to be evaluated is\\

    $f(m)=\sqrt{\dfrac{gm}{c_d}}tanh\Big(\sqrt{\dfrac{gc_d}{m}}t \Big)-v(t)$
    \hfill (E6.4.1)\\

    To apply the Newton-Raphson method, the derivative of this function must be evaluated
    with respect to the unknown, m:\\

    $\dfrac{df(m)}{dm} = \dfrac{1}{2}\sqrt{\dfrac{g}{mc_d}}tanh\Big(\sqrt{\dfrac{gc_d}{m}t} \Big)
    -\dfrac{g}{2m}t$ sech$^2\Big(\sqrt{\dfrac{gc_d}{m}}t \Big)$
    \hfill (E6.4.2)\\

    \begin{figure}[h]
        \begin{lstlisting}[numbers=none]
 function [root,ea,iter]=newtraph(func,dfunc,xr,es,maxit,varargin)
 % newtraph: Newton-Raphson root location zeroes
 %    [root,ea,iter]=newtraph(func,dfunc,xr,es,maxit,p1,p2,...):
 %          uses Newton-Raphson method to find the root of func
 % input:
 %    func = name of function
 %    dfunc = name of derivative of function
 %    xr = initial guess
 %    es = desired relative error (default = 0.0001%)
 %    maxit = maximum allowable iterations (default = 50)
 %    p1,p2,... = additional parameters used by function
 % output:
 %    root = real root
 %    ea = approximate relative error (%)
 %    iter = number of iterations

 if nargin<3,error('at least 3 input arguments required'),end
 if nargin<4|isempty(es),es=0.0001;end
 if nargin<5|isempty(maxit),maxit=50;end
 iter = 0;
 while (1)
    xrold = xr;
    xr = xr - func(xr)/dfunc(xr);
    iter = iter + 1;
    if xr ~= 0, ea = abs((xr - xrold)/xr) * 100; end
    if ea <= es | iter >= maxit, break, end
 end
 root = xr;
        \end{lstlisting}
        \caption{An M-file to implement the Newton-Raphson method.}
    \end{figure}

    \noindent We should mention that although this derivative is not difficult to evaluate in principle, it
    involves a bit of concentration and effort to arrive at the final result.
    
    The two formulas can now be used in conjunction with the function \texttt{newtraph} to
    evaluate the root:\\

    \texttt{>> y = @m sqrt(9.81*m/0.25)*tanh(sqrt(9.81*0.25/m)*4)-36;\\
    \indent >> dy = @m 1/2*sqrt(9.81/(m*0.25))*tanh((9.81*0.25/m)\ldots\\
    \indent\indent \textasciicircum(1/2)*4)-9.81/(2*m)*sech(sqrt(9.81*0.25/m)*4)\textasciicircum2;\\
    \indent >> newtraph(y,dy,140,0.00001)\\
    \indent ans =\\
    \indent\indent 142.7376\\}
\end{example}
\bigskip

\section[SECANT METHODS]{SECANT METHODS}
\noindent As in Example 6.4, a potential problem in implementing the Newton-Raphson method is
the evaluation of the derivative. Although this is not inconvenient for polynomials and
many other functions, there are certain functions whose derivatives may be difficult or
inconvenient to evaluate. For these cases, the derivative can be approximated by a backward
finite divided difference:\\

$f'(x_i)\cong \dfrac{f(x_{i-1})-f(x_i)}{x_{i-1}-x_i}$\\

\noindent This approximation can be substituted into Eq. (6.6) to yield the following iterative
equation:\\

$x_{i+1} = x_i - \dfrac{f(x_i)(x_{i-1}-x_i)}{f(x_{i-1})-f(x_i)}$
\hfill (6.8)\\

Equation (6.8) is the formula for the \emph{secant method}. Notice that the approach requires two
initial estimates of $x$. However, because $f (x)$ is not required to change signs between the
estimates, it is not classified as a bracketing method.

Rather than using two arbitrary values to estimate the derivative, an alternative approach
involves a fractional perturbation of the independent variable to estimate $f'(x)$,\\

$f'(x_i)\cong \dfrac{f(x_i+\delta x_i)-f(x_i)}{\delta x_i}$\\

\noindent where $\delta =$ a small perturbation fraction. This approximation can be substituted into
Eq. (6.6) to yield the following iterative equation:\\

$x_{i+1} = x_i-\dfrac{\delta x_i f(x_i)}{f(x_i + \delta x_i)-f(x_i)}$
\hfill (6.9)\\

We call this the \emph{modified secant method}. As in the following example, it provides a nice
means to attain the efficiency of Newton-Raphson without having to compute derivatives.\\

\begin{example} Modified Secant Method\\

    \noindent\textbf{Problem Statement.}\quad Use the modified secant method to determine the mass of the
    bungee jumper with a drag coefficient of 0.25 kg/m to have a velocity of 36 m/s after 4 s of
    free fall. Note: The acceleration of gravity is $9.81 m/s^2$. Use an initial guess of 50 kg and a
    value of $10^{-6}$ for the perturbation fraction.\\

    \noindent\textbf{Solution.}\quad Inserting the parameters into Eq. (6.9) yields\\

    \noindent First iteration:\\

    $x_0 = 50$\hspace{28.5mm}$f(x_0) = -4.57938708$\\

    $x_0 + \delta x_0 = 50.00005$\hspace{10mm}$f(x_0+\delta x_0) = -4.579381118$\\

    $x_1 = 50-\dfrac{10^{-6}(50)(-4.57938708))}{-4.579381118 - (-4.57938708)}=
    88.39931(\left\lvert \epsilon_t \right\rvert = 38.1\%; \left\lvert \epsilon_a \right\rvert =43.4\% )$\\

    \noindent Second iteration:\\

    $x_1 = 88.39931$\hspace{20mm}$f(x_1) = -1.69220771$\\

    $x_1 + \delta x_1 = 88.39940$\hspace{11mm}$f(x_1+ \delta x_1) = -1.692203516$\\

    $x_2 = 88.39931 - \dfrac{10^{-6}(88.39931)(-1.69220771)}{-1.692203516 - (-1.69220771)} =
    124.08970(\left\lvert \epsilon_t \right\rvert = 13.1\%; \left\lvert \epsilon_a \right\rvert = 28.76\%)$\\

    \noindent The calculation can be continued to yield\\

    \begin{figure}[h]
        \includegraphics[width=0.55\linewidth]{./images/example_6_5_1}
    \end{figure}
    
    The choice of a proper value for $\delta$ is not automatic. If $\delta$ is too small, the method can be
    swamped by round-off error caused by subtractive cancellation in the denominator of
    Eq. (6.9). If it is too big, the technique can become inefficient and even divergent. However,
    if chosen correctly, it provides a nice alternative for cases where evaluating the
    derivative is difficult and developing two initial guesses is inconvenient.
    
    Further, in its most general sense, a univariate function is merely an entity that returns
    a single value in return for values sent to it. Perceived in this sense, functions are not
    always simple formulas like the one-line equations solved in the preceding examples in this
    chapter. For example, a function might consist of many lines of code that could take a significant
    amount of execution time to evaluate. In some cases, the function might even represent
    an independent computer program. For such cases, the secant and modified secant
    methods are valuable.\\
\end{example}
\newpage

\section[BRENT'S METHOD]{BRENT'S METHOD}
\noindent Wouldn't it be nice to have a hybrid approach that combined the reliability of bracketing
with the speed of the open methods? \emph{Brent's root-location method} is a clever algorithm that
does just that by applying a speedy open method wherever possible, but reverting to a reliable
bracketing method if necessary. The approach was developed by Richard Brent (1973)
based on an earlier algorithm of Theodorus Dekker (1969).

The bracketing technique is the trusty bisection method (Sec. 5.4), whereas two different
open methods are employed. The first is the secant method described in Sec. 6.3. As
explained next, the second is inverse quadratic interpolation.\\

\subsection{Inverse Quadratic Interpolation}
\noindent \emph{Inverse quadratic interpolation} is similar in spirit to the secant method. As in Fig. 6.8a, the
secant method is based on computing a straight line that goes through two guesses. The
intersection of this straight line with the x axis represents the new root estimate. For this
reason, it is sometimes referred to as a \emph{linear interpolation method}.

Now suppose that we had three points. In that case, we could determine a quadratic
function of $x$ that goes through the three points (Fig. 6.8b). Just as with the linear secant
method, the intersection of this parabola with the $x$ axis would represent the new root estimate.
And as illustrated in Fig. 6.8b, using a curve rather than a straight line often yields a
better estimate.

Although this would seem to represent a great improvement, the approach has a fundamental
flaw: it is possible that the parabola might not intersect the $x$ axis! Such would be
the case when the resulting parabola had complex roots. This is illustrated by the parabola,
$y = f(x)$, in Fig. 6.9.

The difficulty can be rectified by employing inverse quadratic interpolation. That is,
rather than using a parabola in x, we can fit the points with a parabola in y. This amounts to
reversing the axes and creating a ``sideways'' parabola [the curve, $x = f(y)$, in Fig. 6.9].

If the three points are designated as $(x_{i-2}, y_{i-2})$, $(x_{i-1}, y_{i-1})$, and $(x_i, y_i)$, a quadratic
function of y that passes through the points can be generated as\\

$g(y) = \dfrac{(y-y_{i-1})(y-y_i)}{(y_{i-2}-y_{i-1})(y_{i-2}-y_i)}x_{i-2}+
\dfrac{(y-y_{i-2})(y-y_i)}{(y_{i-1}-y_{i-2})(y_{i-1}-y_i)}x_{i-1}+
\dfrac{(y-y_{i-2})(y-y_{i-1})}{(y_{i}-y_{i-2})(y_{i}-y_{i-1})}x_{i}$
\hfill (6.10)\\
\bigskip

\begin{figure}[h]
    \includegraphics[width=0.8\linewidth]{./images/fig_6_8}
    \caption{Comparison of (a) the secant method and (b) inverse quadratic interpolation. Note that the
    approach in (b) is called ``inverse'' because the quadratic function is written in y rather than in $x$.}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=0.5\linewidth]{./images/fig_6_9}
    \caption{Two parabolas fit to three points. The parabola written as a function of $x, y = f(x)$, has complex
    roots and hence does not intersect the $x$ axis. In contrast, if the variables are reversed, and the
    parabola developed as $x = f (y)$, the function does intersect the $x$ axis.}
\end{figure}
\bigskip

\noindent As we will learn in Sec. 18.2, this form is called a \emph{Lagrange polynomial}. The root, $x_{i+1}$, corresponds
to $y = 0$, which when substituted into Eq. (6.10) yields\\

$x_{i+1}= \dfrac{y_{i-1}y_i}{(y_{i-2} - y_{i-1})(y_{i-2} - y_i)}x_{i-2}+
\dfrac{y_{i-2}y_i}{(y_{i-1} - y_{i-2})(y_{i-1} - y_i)}x_{i-1}+
\dfrac{y_{i-2}y_{i-1}}{(y_{i} - y_{i-2})(y_{i} - y_{i-1})}x_{i}$
\hfill (6.11)\\
\bigskip

\noindent As shown in Fig. 6.9, such a ``sideways'' parabola always intersects the $x$ axis.\\


\begin{example} Inverse Quadratic Interpolation\\

    \noindent\textbf{Problem Statement. }\quad Develop quadratic equations in both $x$ and $y$ for the data points
    depicted in Fig. 6.9: $(1, 2)$, $(2, 1)$, and $(4, 5)$. For the first, $y = f(x)$, employ the quadratic
    formula to illustrate that the roots are complex. For the latter, $x = g(y)$, use inverse quadratic
    interpolation (Eq. 6.11) to determine the root estimate.\\

    \noindent\textbf{Solution.}\quad By reversing the $x$'s and $y$'s, Eq. (6.10) can be used to generate a quadratic in $x$ as\\

    $f(x) = \dfrac{(x-2)(x-4)}{(1-2)(1-4)}2 + \dfrac{(x-1)(x-4)}{(2-1)(2-4)}1 +
    \dfrac{(x-1)(x-2)}{(4-1)(4-2)}5$\\

    \noindent or collecting terms\\

    $f(x)= x^2 - 4x + 5$\\

    \noindent This equation was used to generate the parabola, $y = f(x)$, in Fig. 6.9. The quadratic formula
    can be used to determine that the roots for this case are complex,\\

    $x = \dfrac{4\pm \sqrt{(-4)^2 - 4(1)(5)}}{2} = 2\pm i$\\

    \noindent Equation (6.10) can be used to generate the quadratic in $y$ as\\

    $g(y) = \dfrac{(y-1)(y-5)}{(2-1)(2-5)}1 + \dfrac{(y-2)(y-5)}{(1-2)(1-5)}2 +
    \dfrac{(y-2)(y-1)}{(5-2)(5-1)}4$\\

    \noindent or collecting terms:\\

    $g(y) = 0.5x^2-2.5x+4$\\

    \noindent Finally, Eq. (6.11) can be used to determine the root as\\

    $x_{i+1} = \dfrac{-1(-5)}{(2-1)(2-5)}1 + \dfrac{-2(-5)}{(1-2)(1-5)}2 +
    \dfrac{-2(-1)}{(5-2)(5-1)}4 = 4$\\    
\end{example}

Before proceeding to Brent's algorithm, we need to mention one more case where
inverse quadratic interpolation does not work. If the three $y$ values are not distinct (i.e.,
$y_{i-2} = y_{i-1}$ or $y_{i-1} = y_i$), an inverse quadratic function does not exist. So this is where the
secant method comes into play. If we arrive at a situation where the $y$ values are not distinct,
we can always revert to the less efficient secant method to generate a root using two of
the points. If $y_{i-2} = y_{i-1}$, we use the secant method with $x_{i-1}$ and $x_i$. If $y_{i-1} = y_i$, we use $x_{i-2}$
and $x_{i-1}$.\\

\subsection{Brent's Method Algorithm}
\noindent The general idea behind the \emph{Brent's root-finding method} is whenever possible to use one of
the quick open methods. In the event that these generate an unacceptable result (i.e., a root
estimate that falls outside the bracket), the algorithm reverts to the more conservative
bisection method. Although bisection may be slower, it generates an estimate guaranteed to
fall within the bracket. This process is then repeated until the root is located to within an
acceptable tolerance. As might be expected, bisection typically dominates at first but as the
root is approached, the technique shifts to the faster open methods.

Figure 6.10 presents a function based on a MATLAB M-file developed by Cleve
Moler (2004). It represents a stripped down version of the \texttt{fzero} function which is the professional
root-location function employed in MATLAB. For that reason, we call the
simplified version: \texttt{fzerosimp}. Note that it requires another function \texttt{f} that holds the
equation for which the root is being evaluated.

The \texttt{fzerosimp} function is passed two initial guesses that must bracket the root.
Then, the three variables defining the search interval \texttt{(a,b,c)} are initialized, and \texttt{f} is evaluated
at the endpoints.\\

\begin{figure}[h]
    \begin{lstlisting}[numbers=none]
 function b = fzerosimp(xl,xu)
 a = xl; b = xu; fa = f(a); fb = f(b);
 c = a; fc = fa; d = b - c; e = d;
 while (1)
    if fb == 0, break, end
    if sign(fa) == sign(fb) %If needed, rearrange points
       a = c; fa = fc; d = b - c; e = d;
    end
    if abs(fa) < abs(fb)
       c = b; b = a; a = c;
       fc = fb; fb = fa; fa = fc;
    end
    m = 0.5*(a - b); %Termination test and possible exit
    tol = 2 * eps * max(abs(b), 1);
    if abs(m) <= tol | fb == 0.
       break
    end
    %Choose open methods or bisection
    if abs(e) >= tol & abs(fc) > abs(fb)
       s = fb/fc;
       if a == c %Secant method
          p = 2*m*s;
          q = 1 - s;
       else %Inverse quadratic interpolation
          q = fc/fa; r = fb/fa;
          p = s * (2*m*q * (q - r) - (b - c)*(r - 1));
          q = (q - 1)*(r - 1)*(s - 1);
       end
       if p > 0, q = -q; else p = -p; end;
       if 2*p < 3*m*q - abs(tol*q) & p < abs(0.5*e*q)
          e = d; d = p/q;
       else
          d = m; e = m;
       end
    else %Bisection
       d = m; e = m;
    end
    c = b; fc = fb;
    if abs(d) > tol, b=b+d; else b=b-sign(b-a)*tol; end
    fb = f(b);
 end
    \end{lstlisting}
    \caption{Function for Brent's root-finding algorithm based on a MATLAB M-file developed by Cleve Moler
    (2005).}
\end{figure}

A main loop is then implemented. If necessary, the three points are rearranged to satisfy
the conditions required for the algorithm to work effectively. At this point, if the stopping
criteria are met, the loop is terminated. Otherwise, a decision structure chooses among the
three methods and checks whether the outcome is acceptable. Afinal section then evaluates
\texttt{f} at the new point and the loop is repeated. Once the stopping criteria are met, the loop
terminates and the final root estimate is returned.\\

\section[MATLAB FUNCTION: fzero]{MATLAB FUNCTION: fzero}
\noindent The \texttt{fzero} function is designed to find the real root of a single equation. A simple representation
of its syntax is\\

\texttt{fzero(function, x0)}\\

\noindent where \texttt{function} is the name of the function being evaluated, and \texttt{x0} is the initial guess.
Note that two guesses that bracket the root can be passed as a vector:\\

\texttt{fzero(function,[x0 x1])}\\

\noindent where x0 and x1 are guesses that bracket a sign change.\\

\noindent Here is a simple MATLAB session that solves for the root of a simple quadratic: $x^2 - 9$.
Clearly two roots exist at -3 and 3. To find the negative root:\\

\texttt{>> x = fzero(@(x) x\textasciicircum2-9,-4)\\
\indent x =\\
\indent\indent 3}\\

\noindent If we want to find the positive root, use a guess that is near it:\\

\texttt{>> x = fzero(@(x) x\textasciicircum2-9,4\\)
\indent x =\\
\indent\indent 3}\\

\noindent If we put in an initial guess of zero, it finds the negative root:\\

\texttt{>> x = fzero(@(x) x\textasciicircum2-9,0)\\
\indent x =\\
\indent\indent -3}\\

\noindent If we wanted to ensure that we found the positive root, we could enter two guesses as in\\

\texttt{>> x = fzero(@(x) x\textasciicircum2-9,[0 4])\\
\indent x =\\
\indent\indent 3}\\

\noindent Also, if a sign change does not occur between the two guesses, an error message is displayed\\

\texttt{>> x = fzero(@(x) x\textasciicircum2-9,[-4 4])\\
\indent ??? Error using ==> fzero\\
\indent The function values at the interval endpoints must differ in sign.}\\

\noindent The \texttt{fzero} function works as follows. If a single initial guess is passed, it first performs
a search to identify a sign change. This search differs from the incremental search described
in Section 5.3.1, in that the search starts at the single initial guess and then takes increasingly
bigger steps in both the positive and negative directions until a sign change is detected.

Thereafter, the fast methods (secant and inverse quadratic interpolation) are used unless
an unacceptable result occurs (e.g., the root estimate falls outside the bracket). If an
unacceptable result happens, bisection is implemented until an acceptable root is obtained
with one of the fast methods. As might be expected, bisection typically dominates at first
but as the root is approached, the technique shifts to the faster methods.

A more complete representation of the fzero syntax can be written as\\

\texttt{[x,fx] = fzero(function,x0,options,p1,p2,...)}\\

\noindent where \texttt{[x,fx]} = a vector containing the root \texttt{x} and the function evaluated at the root \texttt{fx},
\texttt{options} is a data structure created by the \texttt{optimset} function, and \texttt{p1, p2}... are any
parameters that the function requires. Note that if you desire to pass in parameters but not
use the \texttt{options}, pass an empty vector [] in its place.

The \texttt{optimset} function has the syntax\\

\texttt{options = optimset('par1',val1,'par2',val2,...)}\\

\noindent where the parameter \texttt{$par_i$} has the value \texttt{$val_i$}. A complete listing of all the possible parameters
can be obtained by merely entering \texttt{optimset} at the command prompt. The parameters
that are commonly used with the \texttt{fzero} function are\\

\texttt{display:} When set to \texttt{'iter'} displays a detailed record of all the iterations.\\
\texttt{tolx:} A positive scalar that sets a termination tolerance on \texttt{x}.

\begin{example} The \texttt{fzero} and \texttt{optimset} Functions\\

    \noindent\textbf{Problem Statement.}\quad Recall that in Example 6.3, we found the positive root of $f (x) = x^{10} - 1$ 
    using the Newton-Raphson method with an initial guess of 0.5. Solve the same problem with \texttt{optimset} and \texttt{fzero}.\\

    \noindent\textbf{Solution.}\quad An interactive MATLAB session can be implemented as follows:\\

    \texttt{>> options = optimset('display','iter');\\
    \indent >> [x,fx] = fzero(@(x) x\textasciicircum10-1,0.5,options)\\}

    %\texttt{\begin{tabular}{c c c c}
    %    Func-count & x & f(x) & Procedure\\
    %    1  &  0.5      & -0.999023 & initial\\
    %    2  &  0.485858 & -0.999267 & search\\
    %    3  &  0.514142 & -0.998709 & search\\
    %    4  &  0.48     & -0.999351 & search\\
    %    5  &  0.52     & -0.998554 & search\\
    %    6  &  0.471716 & -0.999454 & search\\
    %    $\bullet$  &  & & \\
    %    $\bullet$  &  & & \\
    %    $\bullet$  &  & & \\
    %    23 &  0.952548 & -0.385007 & search\\
    %    24 &  -0.14    & -1        & search\\
    %    25 &  1.14     &  2.70722  & search\\
    %\end{tabular}}
    %\bigskip

    %\texttt{\noindent Looking for a zero in the interval [-0.14, 1.14]}\\

    %\texttt{\begin{tabular}{c c c c}
    %    26 & 0.205272 & -1 & interpolation\\
    %    27 & 0.672636 & -0.981042 & bisection\\
    %    28 & 0.906318 & -0.626056 & bisection\\
    %    29 & 1.02316  & 0.257278 & bisection\\
    %    30 & 0.989128 & -0.103551 & interpolation\\
    %    31 & 0.998894 & -0.0110017 & interpolation\\
    %    32 & 1.00001  & 7.68385e-005 & interpolation\\
    %    33 & 1        & -3.83061e-007 & interpolation\\
    %    34 & 1        & -1.3245e-011 & interpolation\\
    %    35 & 1        & 0 & interpolation\\
    %\end{tabular}}
    
    \begin{figure}[h]
        \includegraphics[width=0.6\linewidth]{./images/example_6_7_1}
    \end{figure}

    \texttt{Looking for a zero in the interval [-0.14, 1.14]}\\

    \begin{figure}[h]
        \includegraphics[width=0.6\linewidth]{./images/example_6_7_2}
    \end{figure}

    \texttt{Zero found in the interval: [-0.14, 1.14].\\
    \indent x =\\
    \indent\indent 1\\
    \indent fx =\\
    \indent\indent 0\\}

    Thus, after 25 iterations of searching, \texttt{fzero} finds a sign change. It then uses interpolation
    and bisection until it gets close enough to the root so that interpolation takes over and
    rapidly converges on the root.
    Suppose that we would like to use a less stringent tolerance. We can use the \texttt{optimset}
    function to set a low maximum tolerance and a less accurate estimate of the root results:\\

    \texttt{>> options = optimset ('tolx', 1e-3);\\
    \indent >> [x,fx] = fzero(@(x) x\textasciicircum10-1,0.5,options)\\
    \indent x =\\
    \indent\indent 1.0009\\
    \indent fx =\\
    \indent\indent 0.0090\\}
\end{example}
\bigskip

\newpage
\section[POLYNOMIALS]{POLYNOMIALS}

\noindent Polynomials are a special type of nonlinear algebraic equation of the general form\\

$f_{n}(x)=a_{1} x^{n}+a_{2} x^{n-1}+\cdots+a_{n-1} x^{2}+a_{n} x+a_{n+1}$\hfill (6.12)\\

\noindent where $n$ is the order of the polynomial, and the $a$ 's are constant coefficients. In many (but not all) cases, the coefficients will be real. For such cases, the roots can be real and/or complex. In general, an $n$th order polynomial will have $n$ roots.

Polynomials have many applications in engineering and science. For example, they are used extensively in curve fitting. However, one of their most interesting and powerful applications is in characterizing dynamic systems-and, in particular, linear systems. Examples include reactors, mechanical devices, structures, and electrical circuits.

\subsection{MATLAB Function: \texttt{roots}}

\noindent If you are dealing with a problem where you must determine a single real root of a polynomial, the techniques such as bisection and the Newton-Raphson method can have utility. However, in many cases, engineers desire to determine all the roots, both real and complex. Unfortunately, simple techniques like bisection and Newton-Raphson are not available for determining all the roots of higher-order polynomials. However, MATLAB has an excellent built-in capability, the roots function, for this task.

The roots function has the syntax,
$$
x=\operatorname{roots}(c)
$$
where $x$ is a column vector containing the roots and $c$ is a row vector containing the polynomial's coefficients.

So how does the roots function work? MATLAB is very good at finding the eigenvalues of a matrix. Consequently, the approach is to recast the root evaluation task as eigenvalue problem. Because we will be describing eigenvalue problems later in the book, we will merely provide an overview here.
Suppose we have a polynomial\\

$a_{1} x^{5}+a_{2} x^{4}+a_{3} x^{3}+a_{4} x^{2}+a_{5} x+a_{6}=0$ \hfill (6.13)\\

\noindent Dividing by $a_{1}$ and rearranging yields\\

$x^{5}=-\frac{a_{2}}{a_{1}} x^{4}-\frac{a_{3}}{a_{1}} x^{3}-\frac{a_{4}}{a_{1}} x^{2}-\frac{a_{5}}{a_{1}} x-\frac{a_{6}}{a_{1}}$\\

\noindent A special matrix can be constructed by using the coefficients from the right-hand side as the first row and with 1's and 0's written for the other rows as shown:\\

$
\left[\begin{array}{ccccc}
-a_{2} / a_{1} & -a_{3} / a_{1} & -a_{4} / a_{1} & -a_{5} / a_{1} & -a_{6} / a_{1} \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0
\end{array}\right]
$ \ hfill (6.14)\\

Equation (6.14) is called the polynomial's companion matrix. It has the useful property that its eigenvalues are the roots of the polynomial. Thus, the algorithm underlying the roots function consists of merely setting up the companion matrix and then using MATLAB's powerful eigenvalue evaluation function to determine the roots. Its application, along with some other related polynomial manipulation functions, are described in the following example.

We should note that \texttt{roots} has an inverse function called \texttt{poly}, which when passed the values of the roots, will return the polynomial's coefficients. Its syntax is\\

\texttt{c=poly(r)}\\

\noindent where $r$ is a column vector containing the roots and $c$ is a row vector containing the polynomial's coefficients.
\newpage

\begin{example} Using MATLAB to Manipulate Polynomials and Determine Their Roots\\

    \noindent\textbf{Problem Statement.}\quad Use the following equation to explore how MATLAB can be employed to manipulate polynomials:\\

    $f_{5}(x)=x^{5}-3.5 x^{4}+2.75 x^{3}+2.125 x^{2}-3.875 x+1.25$\hfill (E6.8.1)\\

    \noindent Note that this polynomial has three real roots: $0.5,-1.0$, and 2 ; and one pair of complex roots: $1 \pm 0.5 i$.\\


    \noindent\textbf{Solution.}\quad Polynomials are entered into MATLAB by storing the coefficients as a row vector. For example, entering the following line stores the coefficients in the vector a:\\

    \begin{lstlisting}[numbers=none,frame=none]
        >> a = [1 -3.5 2.75 2.125 -3.875 1.25];
	\end{lstlisting}

    \noindent We can then proceed to manipulate the polynomial. For example we can evaluate it at $x=1$, by typing
    
    \begin{lstlisting}[numbers=none,frame=none]
        >> polyval(a,1)
	\end{lstlisting}
 
    \noindent with the result, $1(1)^{5}-3.5(1)^{4}+2.75(1)^{3}+2.125(1)^{2}-3.875(1)+1.25=-0.25$ :

    \begin{lstlisting}[numbers=none,frame=none]
        ans =
            -0.2500
	\end{lstlisting}

    We can create a quadratic polynomial that has roots corresponding to two of the original roots of Eq. (E6.8.1): $0.5$ and $-1$. This quadratic is $(x-0.5)(x+1)=x^{2}+$ $0.5 x-0.5$. It can be entered into MATLAB as the vector b:

    \begin{lstlisting}[numbers=none,frame=none]
        >> b = [1 .5 -.5]
        b =
             1.0000 0.5000 -0.5000
	\end{lstlisting}


    \noindent Note that the \texttt{poly} function can be used to perform the same task as in

    \begin{lstlisting}[numbers=none,frame=none]
        >> b = poly([0.5 -1])
        b =
                1.0000 0.5000 -0.5000
	\end{lstlisting}

    \noindent We can divide this polynomial into the original polynomial by

    \begin{lstlisting}[numbers=none,frame=none]
            >> [q,r] = deconv(a,b)
	\end{lstlisting}

    \noindent with the result being a quotient (a third-order polynomial, $q$ ) and a remainder ( $r$ )

    \begin{lstlisting}[numbers=none,frame=none]
        q =
            1.0000 -4.0000 5.2500 -2.5000
        r =
            0 0 0 0 0 0
    \end{lstlisting}

    \noindent Because the polynomial is a perfect divisor, the remainder polynomial has zero coefficients. Now, the roots of the quotient polynomial can be determined as
    
    \begin{lstlisting}[numbers=none,frame=none]
        >> x = roots(q)
    \end{lstlisting}

    \noindent with the expected result that the remaining roots of the original polynomial Eq. (E6.8.1) are
    found:\\

    \begin{lstlisting}[numbers=none,frame=none]
        x =
        2.0000
        1.0000 + 0.5000i
        1.0000 - 0.5000i
    \end{lstlisting}

    \noindent We can now multiply $q$ by $b$ to come up with the original polynomial:

    \begin{lstlisting}[numbers=none,frame=none]
        >> a = conv(q,b)
        a =
            1.0000 -3.5000 2.7500 2.1250 -3.8750 1.2500
    \end{lstlisting}

    \noindent We can then determine all the roots of the original polynomial by

    \begin{lstlisting}[numbers=none,frame=none]
        >> x = roots(a)
        x =
            2.0000
            -1.0000
            1.0000 + 0.5000i
            1.0000 - 0.5000i
            0.5000
    \end{lstlisting}

    \noindent Finally, we can return to the original polynomial again by using the \texttt{poly} function:

    \begin{lstlisting}[numbers=none,frame=none]
        >> a = poly(x)
        a =
        1.0000 -3.5000 2.7500 2.1250 -3.8750 1.2500
    \end{lstlisting}

\end{example}

\section{\textbf{CASE STUDY} - PIPE FRICTION}

\textbf{Background.} Determining fluid flow through pipes and tubes has great relevance in
many areas of engineering and science. In engineering, typical applications include the flow
of liquids and gases through pipelines and cooling systems. Scientists are interested in topics
ranging from flow in blood vessels to nutrient transmission through a plant's vascular
system.

The resistance to flow in such conduits is parameterized by a dimensionless number
called the \textit{friction factor}. For turbulent flow, the \textit{Colebrook equation} provides a means to
calculate the friction factor:\medskip

$
0=\frac{1}{\sqrt{f}}+2.0 \log \left(\frac{\varepsilon}{3.7 D}+\frac{2.51}{\operatorname{Re} \sqrt{f}}\right) 
$\hfill (6.15)\\

where $\varepsilon=$ the roughness $(\mathrm{m}), D=$ diameter $(\mathrm{m})$, and $\operatorname{Re}=$ the Reynolds number:
$$
\operatorname{Re}=\frac{\rho V D}{\mu}
$$
where $\rho=$ the fluid's density $\left(\mathrm{kg} / \mathrm{m}^{3}\right), V=$ its velocity $(\mathrm{m} / \mathrm{s})$, and $\mu=$ dynamic viscosity $\left(\mathrm{N} \cdot \mathrm{s} / \mathrm{m}^{2}\right)$. In addition to appearing in Eq. (6.15), the Reynolds number also serves as the criterion for whether flow is turbulent ( $\operatorname{Re}>4000$ ).

In this case study, we will illustrate how the numerical methods covered in this part of the book can be employed to determine $f$ for air flow through a smooth, thin tube. For this case, the parameters are $\rho=1.23 \mathrm{~kg} / \mathrm{m}^{3}, \mu=1.79 \times 10^{-5} \mathrm{~N} \cdot \mathrm{s} / \mathrm{m}^{2}, D=0.005 \mathrm{~m}, V=40 \mathrm{~m} / \mathrm{s}$ and $\varepsilon=0.0015 \mathrm{~mm}$. Note that friction factors range from about $0.008$ to $0.08$. In addition, an explicit formulation called the Swamee-Jain equation provides an approximate estimate:
\bigskip

$$
f=\frac{1.325}{\left[\ln \left(\frac{\varepsilon}{3.7 D}+\frac{5.74}{\operatorname{Re}^{0.9}}\right)\right]^{2}}
$$\hfill (6.16)\\

\noindent\textbf{Solution.} The Reynolds number can be computed as
$$
\operatorname{Re}=\frac{\rho V D}{\mu}=\frac{1.23(40) 0.005}{1.79 \times 10^{-5}}=13,743
$$
This value along with the other parameters can be substituted into Eq. (6.15) to give
$$
g(f)=\frac{1}{\sqrt{f}}+2.0 \log \left(\frac{0.0000015}{3.7(0.005)}+\frac{2.51}{13,743 \sqrt{f}}\right)
$$
Before determining the root, it is advisable to plot the function to estimate initial guesses and to anticipate possible difficulties. This can be done easily with MATLAB:

\begin{lstlisting}[numbers=none,frame=none]
    >> rho=1.23;mu=1.79e-5;D=0.005;V=40;e=0.0015/1000;
    >> Re=rho*V*D/mu;
    >> g=@(f) 1/sqrt(f)+2*log10(e/(3.7*D)+2.51/(Re*sqrt(f)));
    >> fplot(g,[0.008 0.08]),grid,xlabel('f'),ylabel('g(f)')
\end{lstlisting}

As in Fig. 6.11, the root is located at about $0.03$.

Because we are supplied initial guesses ( $x_{l}=0.008$ and $x_{u}=0.08$ ), either of the bracketing methods from Chap. 5 could be used. For example, the bisect function developed in Fig. $5.7$ gives a value of $f=0.0289678$ with a percent relative error of error of $5.926 \times$ $10^{-5}$ in 22 iterations. False position yields a result of similar precision in 26 iterations. Thus, although they produce the correct result, they are somewhat inefficient. This would not be important for a single application, but could become prohibitive if many evaluations were made.

\begin{center}
	\includegraphics[width=0.7\linewidth]{fig_6_11}

	\textsf{Figure 6.11}
\end{center}

We could try to attain improved performance by turning to an open method. Because Eq. (6.15) is relatively straightforward to differentiate, the Newton-Raphson method is a good candidate. For example, using an initial guess at the lower end of the range $\left(x_{0}=\right.$ $0.008$ ), the newt raph function developed in Fig. $6.7$ converges quickly:

\begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> dg=@(f) -2/log(10)*1.255/Re*f^(-3/2)/(e/D/3.7 ...
                            +2.51/Re/sqrt(f))-0.5/f^(3/2);
    >> [f ea iter]=newtraph(g,dg,0.008)
    f =
        0.02896781017144
    ea =
        6.870124190058040e-006
    iter =
        6
\end{lstlisting}

\noindent However, when the initial guess is set at the upper end of the range $\left(x_{0}=0.08\right)$, the routine diverges,

\begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> [f ea iter]=newtraph(g,dg,0.08)
    f =
        NaN + NaNi
\end{lstlisting}

\noindent As can be seen by inspecting Fig. 6.11, this occurs because the function's slope at the initial guess causes the first iteration to jump to a negative value. Further runs demonstrate that for this case, convergence only occurs when the initial guess is below about $0.066$.

So we can see that although the Newton-Raphson is very efficient, it requires good initial guesses. For the Colebrook equation, a good strategy might be to employ the Swamee-Jain equation (Eq. 6.16) to provide the initial guess as in

\begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> fSJ=1.325/log(e/(3.7*D)+5.74/Re^0.9)^2
    fSJ =
        0.02903099711265
    >> [f ea iter]=newtraph(g,dg,fSJ)
    f =
        0.02896781017144
    ea =
        8.510189472800060e-010
    iter =
        3
\end{lstlisting}

Aside from our homemade functions, we can also use MATLAB's built-in \texttt{fzero}
function. However, just as with the Newton-Raphson method, divergence also occurs when
\texttt{fzero} function is used with a single guess. However, in this case, guesses at the lower end
of the range cause problems. For example,

\begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> fzero(g,0.008)

    Exiting fzero: aborting search for an interval containing a sign
    change because complex function value encountered ...
                                    during search.
    (Function value at -0.0028 is -4.92028-20.2423i.)
    Check function or try again with a different starting value.
    ans =
        NaN
\end{lstlisting}

\noindent If the iterations are displayed using optimset (recall Example 6.7), it is revealed that a negative value occurs during the search phase before a sign change is detected and the routine aborts. However, for single initial guesses above about $0.016$, the routine works nicely. For example, for the guess of $0.08$ that caused problems for Newton-Raphson, f zero does just fine:

\begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> fzero(g,0.08)

    ans =
        0.02896781017144
\end{lstlisting}

As a final note, let's see whether convergence is possible for simple fixed-point iteration. The easiest and most straightforward version involves solving for the first $f$ in Eq. (6.15):

$$
f_{i+1}=\frac{0.25}{\left(\log \left(\frac{\varepsilon}{3.7 D}+\frac{2.51}{\operatorname{Re} \sqrt{f_{i}}}\right)\right)^{2}}
$$ \hfill{6.17}

\begin{center}
	\includegraphics[width=0.7\linewidth]{fig_6_12}

	\textsf{Figure 6.12}
\end{center}

The two-curve display of this function depicted indicates a surprising result (Fig. 6.12). Recall that fixed-point iteration converges when the $y_{2}$ curve has a relatively flat slope (i.e., $\left.\left|g^{\prime}(\xi)\right|<1\right)$. As indicated by Fig. $6.12$, the fact that the $y_{2}$ curve is quite flat in the range from $f=0.008$ to $0.08$ means that not only does fixed-point iteration converge, but it converges fairly rapidly! In fact, for initial guesses anywhere between $0.008$ and $0.08$, fixedpoint iteration yields predictions with percent relative errors less than $0.008 \%$ in six or fewer iterations! Thus, this simple approach that requires only one guess and no derivative estimates performs really well for this particular case.

The take-home message from this case study is that even great, professionally developed software like MATLAB is not always foolproof. Further, there is usually no single method that works best for all problems. Sophisticated users understand the strengths and weaknesses of the available numerical techniques. In addition, they understand enough of the underlying theory so that they can effectively deal with situations where a method breaks down.
\\

\noindent\textbf{PROBLEMS}

\begin{multicols}{2}
    \noindent \textbf{6.1} Employ fixed-point iteration to locate the root of
    $$
    f(x)=\sin (\sqrt{x})-x
    $$
    Use an initial guess of $x_{0}=0.5$ and iterate until $\varepsilon_{a} \leq 0.01 \%$. Verify that the process is linearly convergent as described at the end of Sec. 6.1.
    
    \noindent \textbf{6.2}  Use (a) fixed-point iteration and (b) the NewtonRaphson method to determine a root of $f(x)=-0.9 x^{2}+$ $1.7 x+2.5$ using $x_{0}=5$. Perform the computation until $\varepsilon_{a}$ is less than $\varepsilon_{s}=0.01 \%$. Also check your final answer.
   
    \noindent \textbf{6.3}  Determine the highest real root of $f(x)=x^{3}-6 x^{2}+$ $11 x-6.1$ :
    \begin{enumerate}[label=(\alph*)]
        \item Graphically.
        \item  Using the Newton-Raphson method (three iterations, $x_{i}=3.5$ ).
        \item  Using the secant method (three iterations, $x_{i-1}=2.5$ and $x_{i}=3.5$ ).
        \item  Using the modified secant method (three iterations, $x_{i}=3.5, \delta=0.01$ ).
        \item  Determine all the roots with MATLAB.
    \end{enumerate}
    
    \noindent \textbf{6.4}  Determine the lowest positive root of $f(x)=$ $7 \sin (x) e^{-x}-1$ :
    \begin{enumerate}[label=(\alph*)]
        \item  Graphically.
        \item  Using the Newton-Raphson method (three iterations, $x_{i}=0.3$ ).
        \item  Using the secant method (three iterations, $x_{i-1}=0.5$ and $x_{i}=0.4$.
        \item  Using the modified secant method (five iterations, $x_{i}=0.3, \delta=0.01$ ).
    \end{enumerate}
    
    \noindent \textbf{6.5}  Use (a) the Newton-Raphson method and (b) the modified secant method $(\delta=0.05)$ to determine a root of $f(x)=$ $x^{5}-16.05 x^{4}+88.75 x^{3}-192.0375 x^{2}+116.35 x+31.6875$
    using an initial guess of $x=0.5825$ and $\varepsilon_{s}=0.01 \%$. Explain your results.
    
    \noindent \textbf{6.6} Develop an M-file for the secant method. Along with the two initial guesses, pass the function as an argument. Test it by solving Prob. $6.3$.
    
    \noindent \textbf{6.7}  Develop an M-file for the modified secant method. Along with the initial guess and the perturbation fraction, pass the function as an argument. Test it by solving Prob. 6.3.
    
    \noindent \textbf{6.8}  Differentiate Eq. (E6.4.1) to get Eq. (E6.4.2).
    
    \noindent \textbf{6.9}  Employ the Newton-Raphson method to determine a real root for $f(x)=-2+6 x-4 x^{2}+0.5 x^{3}$, using an initial guess of (a) $4.5$, and (b) $4.43$. Discuss and use graphical and analytical methods to explain any peculiarities in your results.

    \noindent \textbf{6.10} The "divide and average" method, an old-time method for approximating the square root of any positive number $a$, can be formulated as
    $$
    x_{i+1}=\frac{x_{i}+a / x_{i}}{2}
    $$
    Prove that this formula is based on the Newton-Raphson algorithm.
    
    \noindent \textbf{6.11} (a) Apply the Newton-Raphson method to the function $f(x)=\tanh \left(x^{2}-9\right)$ to evaluate its known real root at $x=3$. Use an initial guess of $x_{0}=3.2$ and take a minimum of three iterations. (b) Did the method exhibit convergence onto its real root? Sketch the plot with the results for each iteration labeled.
    
    \noindent \textbf{6.12} The polynomial $f(x)=0.0074 x^{4}-0.284 x^{3}+$ $3.355 x^{2}-12.183 x+5$ has a real root between 15 and 20 . Apply the Newton-Raphson method to this function using an initial guess of $x_{0}=16.15$. Explain your results.
    
    \noindent \textbf{6.13} Mechanical engineers, as well as most other engineers, use thermodynamics extensively in their work. The following polynomial can be used to relate the zero-pressure specific heat of dry air $c_{p}$ in $\mathrm{kJ} /(\mathrm{kg} \mathrm{K})$ to temperature in $\mathrm{K}$ :
    $$
    \begin{aligned}
    c_{p}=& 0.99403+1.671 \times 10^{-4} T+9.7215 \times 10^{-8} T^{2} \\
    &-9.5838 \times 10^{-11} T^{3}+1.9520 \times 10^{-14} T^{4}
    \end{aligned}
    $$
    Write a MATLAB script (a) to plot $c_{p}$ versus a range of $T=0$ to $1200 \mathrm{~K}$, and (b) to determine the temperature that corresponds to a specific heat of $1.1 \mathrm{~kJ} /(\mathrm{kg} \mathrm{K}$ ) with MATLAB polynomial functions.
    
    \noindent \textbf{6.14} In a chemical engineering process, water vapor $\left(\mathrm{H}_{2} \mathrm{O}\right)$ is heated to sufficiently high temperatures that a significant portion of the water dissociates, or splits apart, to form oxygen $\left(\mathrm{O}_{2}\right)$ and hydrogen $\left(\mathrm{H}_{2}\right)$ :
    $$
    \mathrm{H}_{2} \mathrm{O} \rightleftarrows \mathrm{H}_{2}+\frac{1}{2} \mathrm{O}_{2}
    $$
    If it is assumed that this is the only reaction involved, the mole fraction $x$ of $\mathrm{H}_{2} \mathrm{O}$ that dissociates can be represented by
    
    $
    K=\frac{x}{1-x} \sqrt{\frac{2 p_{t}}{2+x}}   
    $\hfill{(P6.14.1)}

    where $K$ is the reaction's equilibrium constant and $p_{t}$ is the total pressure of the mixture. If $p_{t}=3 \mathrm{~atm}$ and $K=0.05$, determine the value of $x$ that satisfies Eq. (P6.14.1).

    \noindent \textbf{6.15} The Redlich-Kwong equation of state is given by
    $$
    p=\frac{R T}{v-b}-\frac{a}{v(v+b) \sqrt{T}}
    $$

    where $R=$ the universal gas constant $[=0.518 \mathrm{~kJ} /(\mathrm{kg} \mathrm{K})]$, $T=$ absolute temperature $(\mathrm{K}), p=$ absolute pressure $(\mathrm{kPa})$, and $v=$ the volume of a $\mathrm{kg}$ of gas $\left(\mathrm{m}^{3} / \mathrm{kg}\right)$. The parameters $a$ and $b$ are calculated by
    $$
    a=0.427 \frac{R^{2} T_{c}^{2.5}}{p_{c}} \quad b=0.0866 R \frac{T_{c}}{p_{c}}
    $$
    where $p_{c}=4600 \mathrm{kPa}$ and $T_{c}=191 \mathrm{~K}$. As a chemical engineer, you are asked to determine the amount of methane fuel that can be held in a 3- $\mathrm{m}^{3}$ tank at a temperature of $-40{ }^{\circ} \mathrm{C}$ with a pressure of $65,000 \mathrm{kPa}$. Use a root-locating method of your choice to calculate $v$ and then determine the mass of methane contained in the tank.
    
    \noindent \textbf{6.16}  The volume of liquid $V$ in a hollow horizontal cylinder of radius $r$ and length $L$ is related to the depth of the liquid $h$ by
    $$
    V=\left[r^{2} \cos ^{-1}\left(\frac{r-h}{r}\right)-(r-h) \sqrt{2 r h-h^{2}}\right] L
    $$
    Determine $h$ given $r=2 \mathrm{~m}, L=5 \mathrm{~m}^{3}$, and $V=8 \mathrm{~m}^{3}$. 
    
    \noindent\textbf{6.17}  A catenary cable is one which is hung between two points not in the same vertical line. As depicted in Fig. P6.17a, it is subject to no loads other than its own weight. Thus, its weight acts as a uniform load per unit length along the cable $w(\mathrm{~N} / \mathrm{m})$. A free-body diagram of a section $A B$ is depicted in Fig. P6.17b, where $T_{A}$ and $T_{B}$ are the tension forces at the end. Based on horizontal and vertical force balances, the following differential equation model of the cable can be derived:
    $$
    \frac{d^{2} y}{d x^{2}}=\frac{w}{T_{A}} \sqrt{1+\left(\frac{d y}{d x}\right)^{2}}
    $$

    Calculus can be employed to solve this equation for the height of the cable $y$ as a function of distance $x$ :
    $$
    y=\frac{T_{A}}{w} \cosh \left(\frac{w}{T_{A}} x\right)+y_{0}-\frac{T_{A}}{w}
    $$
    \begin{enumerate}[label=(\alph*)]
        \item Use a numerical method to calculate a value for the parameter $T_{A}$ given values for the parameters $w=10$ and $y_{0}=5$, such that the cable has a height of $y=15$ at $x=50$.
        \item  Develop a plot of $y$ versus $x$ for $x=-50$ to 100 .
    \end{enumerate}

    \begin{center}
        \includegraphics[width=0.7\linewidth]{fig_P6_17}
    
        \textsf{Figure P6.17}
    \end{center}
    
    \noindent\textbf{6.18} An oscillating current in an electric circuit is described by $I=9 e^{-t} \sin (2 \pi t)$, where $t$ is in seconds. Determine all values of $t$ such that $I=3.5$
    
    \noindent\textbf{6.19} Figure P6.19 shows a circuit with a resistor, an inductor, and a capacitor in parallel. Kirchhoff's rules can be used to express the impedance of the system as
    $$
    \frac{1}{Z}=\sqrt{\frac{1}{R^{2}}+\left(\omega C-\frac{1}{\omega L}\right)^{2}}
    $$
    where $Z=$ impedance $(\Omega)$, and $\omega$ is the angular frequency. Find the $\omega$ that results in an impedance of $100 \Omega$ using the fzero function with initial guesses of 1 and 1000 for the following parameters: $R=225 \Omega, C=0.6 \times 10^{-6} \mathrm{~F}$, and $L=0.5 \mathrm{H}$.

    \begin{center}
        \includegraphics[width=0.7\linewidth]{fig_P6_19}
    
        \textsf{Figure P6.19}
    \end{center}

    \noindent\textbf{6.20} Real mechanical systems may involve the deflection of nonlinear springs. In Fig. P6.20, a block of mass $m$ is released a distance $h$ above a nonlinear spring. The resistance force $F$ of the spring is given by
    $$
    F=-\left(k_{1} d+k_{2} d^{3 / 2}\right)
    $$
    Conservation of energy can be used to show that
    $$
    0=\frac{2 k_{2} d^{5 / 2}}{5}+\frac{1}{2} k_{1} d^{2}-m g d-m g h
    $$
    Solve for $d$, given the following parameter values: $k_{1}=$ $40,000 \mathrm{~g} / \mathrm{s}^{2}, k_{2}=40 \mathrm{~g} /\left(\mathrm{s}^{2} \mathrm{~m}^{5}\right), m=95 \mathrm{~g}, \mathrm{~g}=9.81 \mathrm{~m} / \mathrm{s}^{2}$ and $h=0.43 \mathrm{~m}$.
    
    \begin{center}
        \includegraphics[width=0.7\linewidth]{fig_P6_20}
    
        \textsf{Figure P6.20}
    \end{center}

    \noindent\textbf{6.21} Aerospace engineers sometimes compute the trajectories of projectiles such as rockets. A related problem deals with the trajectory of a thrown ball. The trajectory of a ball thrown by a right fielder is defined by the $(x, y)$ coordinates as displayed in Fig. P6.21. The trajectory can be modeled as
    $$
    y=\left(\tan \theta_{0}\right) x-\frac{g}{2 v_{0}^{2} \cos ^{2} \theta_{0}} x^{2}+y_{0}
    $$

    Find the appropriate initial angle $\theta_{0}$, if $v_{0}=30 \mathrm{~m} / \mathrm{s}$, and the distance to the catcher is $90 \mathrm{~m}$. Note that the throw leaves the right fielder's hand at an elevation of $1.8 \mathrm{~m}$ and the catcher receives it at $1 \mathrm{~m}$.
    
    \begin{center}
        \includegraphics[width=0.7\linewidth]{fig_P6_21}
    
        \textsf{Figure P6.21}
    \end{center}

    \noindent\textbf{6.22} You are designing a spherical tank (Fig. P6.22) to hold water for a small village in a developing country. The volume of liquid it can hold can be computed as
    $$
    V=\pi h^{2} \frac{[3 R-h]}{3}
    $$
    where $V=$ volume $\left[\mathrm{m}^{3}\right], h=$ depth of water in tank [m], and $R=$ the tank radius [m].

    If $R=3 \mathrm{~m}$, what depth must the tank be filled to so that it holds $30 \mathrm{~m}^{3}$ ? Use three iterations of the most efficient numerical method possible to determine your answer. Determine the approximate relative error after each iteration. Also, provide justification for your choice of method. Extra information: (a) For bracketing methods, initial guesses of 0 and $R$ will bracket a single root for this example. (b) For open methods, an initial guess of $R$ will always converge. 
    
    \begin{center}
        \includegraphics[width=0.7\linewidth]{fig_P6_22}
    
        \textsf{Figure P6.22}
    \end{center}

    \noindent\textbf{6.23} Perform the identical MATLAB operations as those in Example $6.8$ to manipulate and find all the roots of the polynomial
    $$
    f_{5}(x)=(x+2)(x+5)(x-6)(x-4)(x-8)
    $$

    \noindent\textbf{6.24}In control systems analysis, transfer functions are developed that mathematically relate the dynamics of a system's input to its output. A transfer function for a robotic positioning system is given by
    $$
    G(s)=\frac{C(s)}{N(s)}=\frac{s^{3}+9 s^{2}+26 s+24}{s^{4}+15 s^{3}+77 s^{2}+153 s+90}
    $$

    where $G(s)=$ system gain, $C(s)=$ system output, $N(s)=$ system input, and $s$ = Laplace transform complex frequency. Use MATLAB to find the roots of the numerator and denominator and factor these into the form
    $$
    G(s)=\frac{\left(s+a_{1}\right)\left(s+a_{2}\right)\left(s+a_{3}\right)}{\left(s+b_{1}\right)\left(s+b_{2}\right)\left(s+b_{3}\right)\left(s+b_{4}\right)}
    $$
    where $a_{i}$ and $b_{i}=$ the roots of the numerator and denominator, respectively.
    
    \noindent\textbf{6.25} The Manning equation can be written for a rectangular open channel as
    $$
    Q=\frac{\sqrt{S}(B H)^{5 / 3}}{n(B+2 H)^{2 / 3}}
    $$
    where $Q=$ flow $\left(\mathrm{m}^{3} / \mathrm{s}\right), S=$ slope $(\mathrm{m} / \mathrm{m}), H=\operatorname{depth}(\mathrm{m})$, and $n=$ the Manning roughness coefficient. Develop a fixed-point iteration scheme to solve this equation for $H$ given $Q=5, S=0.0002, B=20$, and $n=0.03$. Perform the computation until $\varepsilon_{\alpha}$ is less than $\varepsilon_{s}=0.05 \%$. Prove that your scheme converges for all initial guesses greater than or equal to zero.
    
    \noindent\textbf{6.26} See if you can develop a foolproof function to compute the friction factor based on the Colebrook equation as described in Sec. 6.7. Your function should retum a precise result for Reynolds number ranging from 4000 to $10^{7}$ and for $\varepsilon / D$ ranging from $0.00001$ to $0.05$.
    
    \noindent\textbf{6.27}Use the Newton-Raphson method to find the root of $f(x)=e^{-0.5 x}(4-x)-2$

    Employ initial guesses of (a) 2 , (b) 6 , and (c) 8 . Explain your results.
    
    \noindent\textbf{6.28} Given
    $$
    f(x)=-2 x^{6}-1.5 x^{4}+10 x+2
    $$
    Use a root-location technique to determine the maximum of this function. Perform iterations until the approximate relative error falls below $5 \%$. If you use a bracketing method, use initial guesses of $x_{l}=0$ and $x_{u}=1$. If you use the Newton-Raphson or the modified secant method, use an initial guess of $x_{i}=1$. If you use the secant method, use initial

    guesses of $x_{i-1}=0$ and $x_{i}=1$. Assuming that convergence is not an issue, choose the technique that is best suited to this problem. Justify your choice.
    
    \noindent\textbf{6.29} You must determine the root of the following easily differentiable function:
    $$
    e^{0.5 x}=5-5 x
    $$
    Pick the best numerical technique, justify your choice, and then use that technique to determine the root. Note that it is known that for positive initial guesses, all techniques except fixed-point iteration will eventually converge. Perform iterations until the approximate relative error falls below $2 \%$. If you use a bracketing method, use initial guesses of $x_{I}=0$ and $x_{u}=2$. If you use the Newton-Raphson or the modified secant method, use an initial guess of $x_{i}=0.7$. If you use the secant method, use initial guesses of $x_{i-1}=0$ and $x_{i}=2$.
   
    \noindent\textbf{6.30} (a) Develop an M-file function to implement Brent's root-location method. Base your function on Fig. 6.10, but with the beginning of the function changed to
    \begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    function [b,fb] = fzeronew(f,xl,xu,varargin)
    % fzeronew: Brent root location zeroes
    % [b,fb] = fzeronew(f,xl,xu,p1,p2,...):
    % uses Brent's method to find the root of f
    % input:
    % f = name of function
    % xl, xu = lower and upper guesses
    % p1,p2,... = additional parameters used by f
    % output:
    % b = real root
    % fb = function value at root
    \end{lstlisting}
    
    Make the appropriate modifications so that the function performs as outlined in the documentation statements. In addition, include error traps to ensure that the function's three required arguments \texttt{(f, x 1, x u)} are prescribed, and that the initial guesses bracket a root.
    (b) Test your function by using it to solve for the root of the function from Example $5.6$ using
    
    \begin{lstlisting}[numbers=none,frame=none,xleftmargin=-\leftmargini]
    >> [x,fx] = fzeronew(@(x,n) x^n-1,0,1.3,10)
    \end{lstlisting}

\end{multicols}

\end{document}